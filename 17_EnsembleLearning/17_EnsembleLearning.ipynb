{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddce491",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "Ensemble learning is een manier om meerdere machine learning algorithmes te gaan combineren om een beter resultaat te bekomen.\n",
    "Een voorbeeld hiervan is Random Forest dat een ensemble is van een aantal decision trees. \n",
    "Een belangrijke opmerking hierbij is dat meerdere soorten machine learning algoritmes gecombineerd kunnen worden.\n",
    "Er zijn hier drie bekende varianten van, namelijk:\n",
    "* Stacking\n",
    "* Bagging\n",
    "* Boosting\n",
    "\n",
    "Deze varianten gaan we nu 1 voor 1 behandelen.\n",
    "\n",
    "# Stacking \n",
    "\n",
    "Een schets van hoe een stacking-ensemble werkt zie je hieronder.\n",
    "Bij de stacking-methode wordt de volledige trainingsdata gebruikt om meerdere modellen te trainen.\n",
    "Deze kunnen maar moeten niet van hetzelfde algoritme zijn en ook de hyperparameters kunnen verschillen.\n",
    "Nadat al deze modellen getrained zijn, wordt er nog model getrained dat een selectie maakt van wat de uiteindelijke voorspelling is op basis van de uitkomst van alle getrainde modellen.\n",
    "Dit tweede model kan dus evalueren wanneer welk model het meest correct is.\n",
    "Het algoritme hiervoor kan vrij gekozen worden en kan zelfs een ander ensemble zijn.\n",
    "Bij stacking is dus niet strikt majority voting van toepassing.\n",
    "\n",
    "![ensemble](images\\stacking.png)\n",
    "\n",
    "Een [stacking ensemble](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) kan als volgt geimplementeerd worden in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36d42fd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc train data 0.9916666666666667\n",
      "Acc test data 0.9\n",
      "{'scaler__with_std': False, 'stack__final_estimator': LogisticRegression(), 'stack__tree1__max_depth': 4, 'stack__tree2__max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load iris dataset, split in test en train, perform scaling on X_train en X_test\n",
    "X,y = load_iris(return_X_y=True, as_frame=True)\n",
    "#display(X)\n",
    "#display(y)\n",
    "\n",
    "# split in train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size =0.2)\n",
    "\n",
    "# create StackingClassifier bestaande uit random forest, logistic regression, svm, knn\n",
    "estimators = [(\"logReg\", LogisticRegression(C=1)), \n",
    "              (\"SVM\", SVC()), \n",
    "              (\"tree2\", DecisionTreeClassifier()), \n",
    "              (\"tree1\", DecisionTreeClassifier())]\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "Column\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"stack\", stacking_clf)\n",
    "])\n",
    "\n",
    "parameters = [{\n",
    "    \"scaler__with_std\" : [True, False],\n",
    "    \"stack__tree1__max_depth\" : [4,8,12],\n",
    "    \"stack__tree2__max_depth\" : [3,6,9],\n",
    "    \"stack__final_estimator\": [LogisticRegression(), SVC()]\n",
    "}]\n",
    "\n",
    "searcher = GridSearchCV(pipe, param_grid=parameters)\n",
    "\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# print score van algemeen ensemble en van individuele parameters\n",
    "print(\"Acc train data\", searcher.score(X_train, y_train))\n",
    "print(\"Acc test data\", searcher.score(X_test, y_test))\n",
    "\n",
    "print(searcher.best_params_)\n",
    "\n",
    "#print(searcher.best_estimator_.named_steps[\"stack\"].estimators_[0].score(X_test, y_test))\n",
    "#print(searcher.best_estimator_.named_steps[\"stack\"].estimators_[1].score(X_test, y_test))\n",
    "#print(searcher.best_estimator_.named_steps[\"stack\"].estimators_[2].score(X_test, y_test))\n",
    "#print(searcher.best_estimator_.named_steps[\"stack\"].estimators_[3].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8dc6a",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "De tweede methode bagging is ook gekend onder de naam bootstrap bagging.\n",
    "Dit houdt in dat elk model slechts getrained wordt op een deel van de data (bootstrapped samples).\n",
    "Deze observaties om elk model te trainen worden willekeurig gekozen met teruglegging.\n",
    "Typisch wordt er ongeveer 60% van de totale data gebruikt om elk model te trainen.\n",
    "Wanneer elk model getrained is wordt er een standaard majority voting toegepast om de uiteindelijke predictie te bekomen (bij regressie wordt er het gemiddelde genomen).\n",
    "\n",
    "![bagging](images\\bagging.png)\n",
    "\n",
    "Een belangrijke opmerking is dat theoretisch gezien meerdere types van machine learning algoritmes gecombineerd kunnen worden maar dit dit niet rechtstreeks gaat met sklearn zonder zelf een ensemble te implementeren. \n",
    "Dit is ook niet nodig omdat zolang je ensemble groot genoeg is, je elke gewenste accuraatheid kan bereiken.\n",
    "Een implementatie van de [bagging methode](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced313ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n",
      "Wall time: 4 ms\n",
      "0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017005205154418945"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(base_estimator=SVC(), n_estimators=10, max_samples=0.6)\n",
    "\n",
    "# train en predict op de iris dataset hou tijd bij met %time of time.time()\n",
    "%time bag.fit(X_train, y_train)\n",
    "%time bag.score(X_test, y_test)\n",
    "\n",
    "print(bag.score(X_test, y_test))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "bag.fit(X_train, y_train)\n",
    "time.time() - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c54da",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "De derde variant van ensemble learning methoden is boosting.\n",
    "Dit is een aanpassing van de bagging methode waar de bootstrapped samples niet meer willekeurig zijn.\n",
    "Bij boosting worden de modellen sequentieel getrained en gevalideerd (met de trainingsdata diet niet in de bootstrapped sample zit). \n",
    "De classificaties die verkeerd waren bij deze validatie stap hebben een grotere kans om in de bootstrapped sample te zitten van het volgende model.\n",
    "Omdat de uitkomst van elk model nodig is voor het volgende kan dit niet geparallelliseerd worden waardoor de trainingstijd snel kan oplopen.\n",
    "Het voordeel echter van deze methode is dat de accuraatheid van het gecombineerde model hoger gaat zijn dan bij bagging.\n",
    "De meest bekende implementatie van deze methode wordt AdaBoost genoemd.\n",
    "Daarnaast wordt tegenwoordig ook XGBoost benoemd wat staat voor Extreme Gradient Boosting.\n",
    "Het probleem is echter dat deze techniek niet standaard in sklearn staat.\n",
    "Hiervoor moet een extra package toegevoegd worden, meer informatie hierover vind je [hier](https://towardsdatascience.com/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97).\n",
    "Deze techniek is een speciale variant van een random forest (er wordt gewerkt met meerdere decision trees).\n",
    "Meer informatie over de api vind je [hier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier).\n",
    "De voordelen van xgboost zijn:\n",
    "* Hoge accuraatheid\n",
    "* Heel snelle uitvoering door parallellisatie\n",
    "* Flexibel algoritme door keuse van optimalisatie\n",
    "* Kan omgaan met missing data\n",
    "* Voert autmatisch pruning uit om overfitting tegen te gaan\n",
    "* Ingebouwde cross-validatie\n",
    "\n",
    "![boosting](images\\boosting.png)\n",
    "\n",
    " Een implementatie van de [boosting methode](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d728e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n",
      "Wall time: 0 ns\n",
      "0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0029942989349365234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "\n",
    "# train en predict op de iris dataset hou tijd bij met %time of time.time()\n",
    "%time boost.fit(X_train, y_train)\n",
    "%time boost.score(X_test, y_test)\n",
    "\n",
    "print(boost.score(X_test, y_test))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "boost.fit(X_train, y_train)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eebbd1",
   "metadata": {},
   "source": [
    "Meer informatie over alle mogelijke ensemble-methoden in sklearn vind je [hier](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943df106",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Gradient Boosting is een ML-techniek die iteratief modellen toevoegt aan een ensemble.\n",
    "Er wordt begonnen met een eenvoudig model dat zeer naief is.\n",
    "De voorspellingen van dit model worden gebruikt om een kost-functie te berekenen.\n",
    "Deze functie wordt dan gebruikt voor een nieuw model te trainen dat toegevoegd aan het ensemble.\n",
    "De parameters van dit nieuwe model worden zo gekozen dat de kost-functie verminderd.\n",
    "Met dit uitgebreide ensemble worden opnieuw voorspelling gedaan die het dan terug gebruikt worden om een kostfunctie te berekenen.\n",
    "\n",
    "De belangrijkste parameters van deze techniek zijn:\n",
    "* n_estimators: Geeft het maximum aantal bomen in het ensemble weer. Een lage waarde kan leiden tot underfitting, een grote tot overfitting.\n",
    "* learning rate: Dit kan gebruikt zorden om de impact van extra bomen te verkleinen om overfitting tegen te gaan. Lagere waarden gaan normaal een hogere accuraatheid maar ook een hogere trainingstijd opleveren.\n",
    "* early stopping: Deze parameter moet meegegeven worden bij het fitten en kan gebruikt worden om overfitting tegen te gaan. Indien de validation score niet afneemt gedurende dit aantal rondes, dan stopt de fit-functie omdat er overfitting gedetecteerd wordt.\n",
    "* eval_set: Deze parameter is een tuple van de validatieset dat gebruikt kan worden voor het early-stopping te testen. Deze moet dus met early-stopping gecombineerd worden.\n",
    "* n_jobs: aantal cores dat kan gebruikt worden voor de training\n",
    "\n",
    "\n",
    "Meer informatie over de parameters van deze techniek vind je [hier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ad8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install xgboost als niet gekend\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5dc1c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:58:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.baetens3\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create xgboost classifier\n",
    "from xgboost import XGBClassifier\n",
    "xgbooster = XGBClassifier(n_estimators=10000, learning_rate=0.1)\n",
    "\n",
    "# train en predict op de iris dataset hou tijd bij met %time of time.time()\n",
    "xgbooster.fit(X_train, y_train, early_stopping_rounds=5, \n",
    "             eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "xgbooster.score(X_test, y_test)\n",
    "\n",
    "# probleem met gridsearch door de parameters in de fit() \n",
    "# -> early stopping en eval_set proberen kan niet geautomatiseerd worden\n",
    "# early stopping rounds in nieuwere versie in constructor maar de eval_set nog niet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003376a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
