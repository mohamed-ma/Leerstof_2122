{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc0c66e",
   "metadata": {},
   "source": [
    "# Neurale netwerken\n",
    "\n",
    "De twee meest gebruikte packages voor het werken met neurale netwerken zijn Tensorflow (met bovenliggende keras) en PyTorch.\n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "Open-source library ontwikkeld door Google voor te opbouwen, trainen en gebruiken van Neurale Netwerken. Bovenop Tensorflow is een keras package geschreven voor het gebruiksgemak.\n",
    "De keras package is modulair en uitbreidbaar en maakt het mogelijk om complexe neurale netwerken te bouwen zoas voor deep learning.\n",
    "\n",
    "Meer informatie over tensorflow en keras vind je respectievelijk [hier](https://www.tensorflow.org/), [hier](https://keras.io/) en [hier](https://keras.io/#getting-started-30-seconds-to-keras).\n",
    "\n",
    "### Installatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92582f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db379c",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b3baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981c890",
   "metadata": {},
   "source": [
    "### Creating FeedForward Neural Network\n",
    "\n",
    "Meer informatie over het Sequential class vind je [hier](https://keras.io/api/models/sequential/). Deze klasse kan gebruikt worden om een neuraal netwerk op te bouwen maar ook voor preprocessing te doen.\n",
    "Het stelt namelijk een stapel van lagen voor die achter elkaar uitgevoerd worden.\n",
    "\n",
    "Meer informatie over het Dense layer vind je [hier](https://keras.io/api/layers/core_layers/dense/). Dit is een standaard laag waar elk neuron van de input verbonden is met elk neuron van de output.\n",
    "Je kan de activation functie meegeven als argument aan deze laag of als aparte laag (zie [hier](https://keras.io/api/layers/activations/)).\n",
    "\n",
    "De dropout laag valt onder de [regularisatie categorie](https://keras.io/api/layers/regularization_layers/). De belangrijkste parameter hierbij is de rate, die het percent neuronen aangeeft dat willekeurig op 0 staat. \n",
    "\n",
    "In de compile functie kun je specificeren hoe de kost berekend wordt, de optimizer voor de learning rate, etc ([alle argumenten](https://keras.io/api/models/model_training_apis/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dc2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10 # afhankelijk van je data\n",
    "n_neurons_hidden_1 = 50\n",
    "n_neurons_hidden_2 = 50\n",
    "n_outputs = 5 # afhankelijk van je wil bereiken\n",
    "\n",
    "# maak het model aan\n",
    "model = Sequential() # Feed forward\n",
    "model.add(Dense(units=n_neurons_hidden_1, input_dim=n_features, activation='relu')) # hidden laag 1\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=n_neurons_hidden_2, input_dim=n_neurons_hidden_1, activation='relu')) # hidden laag 2\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=n_outputs, input_dim=n_neurons_hidden_2, activation='sigmoid')) # output laag\n",
    "\n",
    "# compileer/initialiseer het model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# trainen/fitten\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "#predict\n",
    "model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b5815",
   "metadata": {},
   "source": [
    "Een aantal belangrijke opmerkingen hierbij zijn:\n",
    "* Bij de eerste Dense-laag is het belangrijk om het aantal inputs mee te geven\n",
    "* De beschikbare activatie functies zijn\n",
    " * softmax\n",
    " * relu\n",
    " * sigmoid\n",
    " * tanh\n",
    " * linear\n",
    "* Er zijn een aantal optimizers mogelijk binnen sklearn. De twee veruit meest gebruikte zijn SGD (gecombineerd met Nesterov) en Adam.\n",
    " * SGD: Standaard Gradient Descent met vaste learning rate maar met momentum om uit lokale optima te geraken\n",
    " * Adam: Automatisch tweaken van learning rate\n",
    " * RMSProp: Meer gebruikt bij RNN\n",
    " * Adagrad\n",
    " * Adamax\n",
    "* Epochs: Aantal keer dat de volledige trainingsset door het neuraal netwerk verwerkt wordt.\n",
    "* Batch-size: Hoeveel samples er verwerkt worden voor de gewichten geupdate worden. \n",
    "  * Batch mode: Volledige trainingsset voor update gewichten (Veel geheugen nodig)\n",
    "  * Stochastic Mode: Na elk sample is er een update (Duurt lang voor convergentie\n",
    "  * Mini-batch mode: Alles ertussen in (32 is vaak een goed optie)\n",
    "\n",
    "In deze blok is er gekozen voor de Categorical Crossentropy loss-functie om de error te berekenen.\n",
    "Er zijn er echter nog een heel aantal loss-functies die in sommige gevallen betere resultaten geven.\n",
    "De meeste gebruikte zijn:\n",
    "* Regressie:\n",
    " * Mean Squared Error: Veruit de meest gebruikte, vooral voor Gaussianse verdelingen.\n",
    " * Mean Squared Logistic Error: Straft fouten veel minder sterk af, vooral voor te werken met niet geschaalde features\n",
    " * Mean Absolute Error Loss: Meer robust voor outliers\n",
    "* Classificatie (2 of meer klassen)\n",
    " * Binary Cross-Entropy Loss: Standaard en meest gebruikte, output-laag moet sigmoid activation function\n",
    " * Categorical Cross Entropy: softmax om uit een reeks klassen 1 klasse met hoge zekerheid te voorspellen\n",
    " * Hinge Loss: Kan gebruikt worden als alternatief voor SVM-modellen, activation functie moet tangens hyperbolische functie zijn\n",
    " * Squared Hinge Loss: Alternatief voor hinge-loss dat smoothere error functie geeft.\n",
    "* Multi-label classification: meerdere klassen tegelijkertijd mogelijk \n",
    " * binary cross entropy: met sigmoids\n",
    " \n",
    " Daarnaast moet ook de metriek bepaald worden om de correctheid van het model te bepalen. Hiervoor zijn de volgende opties:\n",
    "* Regression:\n",
    " * Mean Squared Error\n",
    " * Mean Absolute Error\n",
    " * Mean Absolute Percentage Error\n",
    " * Cosine Proximity\n",
    "* Classification\n",
    " * Binary Accuracy\n",
    " * Categorical Accuracy\n",
    " * Top k Categorical Accuracy\n",
    " * Cosine Proximity\n",
    " \n",
    " Om af te ronden is er ook nog een parameter **validation_split** bij de fit-functie om een deel apart te houden van de trainingsdata.\n",
    "\n",
    "### Save en load models\n",
    "\n",
    "Bij het werken met Neurale Netwerken zal het snel duidelijk worden dat het training snel lang kan duren omdat er duizenden parameters getrained moeten worden.\n",
    "Hierdoor is het aangeraden om een model op te slaan zodat niet steeds hetzelfde model moet getrained worden.\n",
    "Dit kan zelf gedaan worden tijdens het trainen zodat indien er iets misgaat er kan verder getrained worden zonder van in het begin te moeten beginnen.\n",
    "Het bewaren en inladen van een model kan met onderstaande code.\n",
    "Hoe dit te doen kan je [hier](https://www.tensorflow.org/tutorials/keras/save_and_load) vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f41084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enkel gewichten bewaren\n",
    "model.save_weights(\"path\")\n",
    "model.load_weights(\"path\")\n",
    "\n",
    "# volledig model op te slaan (ook architectuur en parameters) (bijvoorbeeld bij training om backups te maken)\n",
    "model.save(\"path naar model\")\n",
    "model = load_model(\"path naar model\")\n",
    "\n",
    "# bewaar automatisch na elke epoch\n",
    "cb = ModelCheckpoint(filepath=\"path\", save_weights_only=True, verbose=1)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1f87d",
   "metadata": {},
   "source": [
    "Bekijk [dit voorbeeld](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/) over werken met dataframes en [dit voorbeeld](https://keras.io/examples/nlp/text_classification_from_scratch/) voor nlp.\n",
    "Bestudeer de gevolgde stappen en verschillende onderdelen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2322ec",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "Dit is net zoals Tensorflow een open-source package om te werken met neurale netwerken. \n",
    "Deze package is tijdens tensorflow versie 1 populair geworden bij onderzoeksgroepen omdat het gebruiksvriendelijker was.\n",
    "Het is ook flexibeler/eenvoudig om complexere zaken die geen standaardoplossing hebben mee uit te voeren.\n",
    "Met tensorflow versie 2 is het wel gemakkelijker om standaard oplossingen te bekomen, echter is het moeilijker om iets buiten de standaard uit te voeren.\n",
    "\n",
    "Een belangrijk verschil is dat pytorch gebruik maakt van Tensors ipv numpy arrays.\n",
    "De functionaliteit tussen de twee is gelijk maar de Tensors kunnen uitgevoerd worden op een GPU indien gewenst en werken samen met CUDA.\n",
    "Bij tensorflow gebeurt dit meer in de achtergrond.\n",
    "\n",
    "### Installation\n",
    "\n",
    "De basis installatie van PyTorch is even eenvoudig als Tensorflow.\n",
    "Echter indien er gewenst is dat de Neurale Netwerken getrained en uitgevoerd worden met behulp van een GPU is het commando iets ingewikkelder.\n",
    "Meer informatie over het correcte commando om PyTorch te installeren vind je [hier](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd93de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ab830",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4ecc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9389be7",
   "metadata": {},
   "source": [
    "### Creating FeedForward Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8887e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-5359fe5527ee>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-5359fe5527ee>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    X_train = torch.FloatTensor()X_train = torch.FloatTensor()\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# alles binnen pytorch werkt met tensors wat een andere naam is voor een feature vector maar conversie is nodig\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call constructor of nn.Module\n",
    "        super().init__()\n",
    "        \n",
    "        # make the necessary layers\n",
    "        self.hidden1 = nn.Linear(in_features=10, out_features=50)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.hidden2 = nn.Linear(in_features=50, out_features=50)\n",
    "        self.output = nn.Linear(in_features=50, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # w1 * x1 + w2 * x2 + ...\n",
    "        x = self.hidden1(x)\n",
    "        \n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # laag 2\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # output\n",
    "        x = F.sigmoid(self.output(x))\n",
    "        \n",
    "model = NeuralNetwork()\n",
    "lossFunctie = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 50\n",
    "loss = []\n",
    "\n",
    "# fitting\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train)\n",
    "    loss_tmp = lossFunctie(y_hat, y_train)\n",
    "    loss.append(loss_tmp)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_tmp.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# predictie\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for val in X_test:\n",
    "        y_hat = model.forward(val)\n",
    "        # dit plaats vijf waarden in de lijst\n",
    "        predictions.append(y_hat)\n",
    "        \n",
    "        #dit plaats enkel de klasse met de hoogste kans\n",
    "        predictions.append(y_hat.argmax().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87f8b",
   "metadata": {},
   "source": [
    "### Save and load models\n",
    "\n",
    "Bij tensorflow moest er gekozen worden tussen enkel de weights opslaan of het volledig model opslaan.\n",
    "Bij Pytorch worden typisch enkel de gewichten opgeslaan en moet de definitie van de klasse (wat de architectuur bepaald) ook gekend zijn om de gewichten terug correct te kunnen laden.\n",
    "Hier gaan we niet dieper op in maar meer informatie kan je [hier](https://pytorch.org/tutorials/beginner/saving_loading_models.html) vinden.\n",
    "\n",
    "## Oefening\n",
    "\n",
    "Download de MNIST dataset van sklearn (load_digits) dat 8x8 pixels bevat en beantwoord de volgende vragen:\n",
    "* Maak een Neuraal Netwerk aan dat bestaat uit 1 laag met 20 neuronen zonder dropout. Hoeveel inputs en outputs heeft het model? Hoeveel gewichten moeten er getrained worden. Hoe accuraat is het model?\n",
    "* Maak een NN aan dat bestaat uit 3 lagen van 30 neuronen zonder dropout. Vergelijk dit model met het vorige? Is er een model aan het under/over-fitten. Waaraan merk je dit? Hoe kan je dit oplossen?\n",
    "* Ga op zoek naar het model met de hoogste accuraatheid. Welke architectuur en hyperparameters heeft dit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306ed6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
